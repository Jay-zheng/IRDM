{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Loading Libs\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loading two pre defined functions/dictionary \n",
    "from spell_check_dict import spelling_checker_dict as spellcheck_dict\n",
    "#this dictionary has been downloaded from Kaggel forum\n",
    "\n",
    "\n",
    "from google_search import spell_check as spellcheck\n",
    "# google_search correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vinyl grip strip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck_dict['vynal grip strip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vinyl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellcheck('vinyal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('test.csv', encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv('product_descriptions.csv')\n",
    "df_attr = pd.read_csv('attributes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dummy dataframes for proof of concept\n",
    "df_train_1 = df_train.copy()\n",
    "df_test_1 = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cleaning Train Data---\n",
      " :) Train Data Cleaning Finished in 1.4 minutes\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning functions start here\n",
    "\n",
    "# helol wold -> hello world\n",
    "# only uses predefined spell check dict atm\n",
    "# we can include google spell check for those not in dict\n",
    "def spellchecker(searchwords):\n",
    "    if searchwords in spellcheck_dict.keys():\n",
    "        return spellcheck_dict[searchwords]\n",
    "    else:\n",
    "        return searchwords\n",
    "\n",
    "# Hello World -> hello world\n",
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# process all regexes from other functions below\n",
    "def regex_processor(text, replace_list):\n",
    "    for pattern, replace in replace_list:\n",
    "            try:\n",
    "                text = re.sub(pattern, replace, text)\n",
    "            except:\n",
    "                pass\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip() \n",
    "\n",
    "# 200 wattage, 200 watts, 200 watt -> 200 watt\n",
    "def convertunits(text):\n",
    "    replace_list = [\n",
    "            (r\"([0-9]+)( *)(inches|inch|in|in.|')\\.?\", r\"\\1 in. \"),\n",
    "            (r\"([0-9]+)( *)(pounds|pound|lbs|lb|lb.)\\.?\", r\"\\1 lb. \"),\n",
    "            (r\"([0-9]+)( *)(foot|feet|ft|ft.|'')\\.?\", r\"\\1 ft. \"),\n",
    "            (r\"([0-9]+)( *)(square|sq|sq.) ?\\.?(inches|inch|in|in.|')\\.?\", r\"\\1 sq.in. \"),\n",
    "            (r\"([0-9]+)( *)(square|sq|sq.) ?\\.?(feet|foot|ft|ft.|'')\\.?\", r\"\\1 sq.ft. \"),\n",
    "            (r\"([0-9]+)( *)(cubic|cu|cu.) ?\\.?(inches|inch|in|in.|')\\.?\", r\"\\1 cu.in. \"),\n",
    "            (r\"([0-9]+)( *)(cubic|cu|cu.) ?\\.?(feet|foot|ft|ft.|'')\\.?\", r\"\\1 cu.ft. \"),\n",
    "            (r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1 gal. \"),\n",
    "            (r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1 oz. \"),\n",
    "            (r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1 cm. \"),\n",
    "            (r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1 mm. \"),\n",
    "            (r\"([0-9]+)( *)(minutes|minute)\\.?\", r\"\\1 min. \"),\n",
    "            (r\"([0-9]+)( *)(Â°|degrees|degree)\\.?\", r\"\\1 deg. \"),\n",
    "            (r\"([0-9]+)( *)(v|volts|volt)\\.?\", r\"\\1 volt. \"),\n",
    "            (r\"([0-9]+)( *)(wattage|watts|watt)\\.?\", r\"\\1 watt. \"),\n",
    "            (r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1 amp. \"),\n",
    "            (r\"([0-9]+)( *)(qquart|quart)\\.?\", r\"\\1 qt. \"),\n",
    "            (r\"([0-9]+)( *)(hours|hour|hrs.)\\.?\", r\"\\1 hr \"),\n",
    "            (r\"([0-9]+)( *)(gallons per minute|gallon per minute|gal per minute|gallons/min.|gallons/min)\\.?\", r\"\\1 gal. per min. \"),\n",
    "            (r\"([0-9]+)( *)(gallons per hour|gallon per hour|gal per hour|gallons/hour|gallons/hr)\\.?\", r\"\\1 gal. per hr \"),\n",
    "        ]\n",
    "    return regex_processor(text, replace_list)   \n",
    "\n",
    "\n",
    "# helloWorld -> hello World\n",
    "def splitcases(text):\n",
    "    replace_list = [\n",
    "            (r\"(\\w)[\\.?!]([A-Z])\", r\"\\1 \\2\"),\n",
    "            (r\"(?<=( ))([a-z]+)([A-Z]+)\", r\"\\2 \\3\"),\n",
    "        ]\n",
    "    return regex_processor(text, replace_list)   \n",
    "    \n",
    "\n",
    "# hello/world, hello-world -> hello world\n",
    "def removewordsplitters(text):\n",
    "    replace_list = [\n",
    "            (r\"([a-zA-Z]+)[/\\-]([a-zA-Z]+)\", r\"\\1 \\2\"),\n",
    "        ]\n",
    "    return regex_processor(text, replace_list)   \n",
    "    \n",
    "# 1x1 -> 1 x 1\n",
    "def digitsplitters(text):\n",
    "    replace_list = [\n",
    "            (r\"(\\d+)[\\.\\-]*([a-zA-Z]+)\", r\"\\1 \\2\"),\n",
    "            (r\"([a-zA-Z]+)[\\.\\-]*(\\d+)\", r\"\\1 \\2\"),\n",
    "        ]\n",
    "    return regex_processor(text, replace_list)   \n",
    "\n",
    "# 1,000 -> 1000\n",
    "def digitcommaremover(text):\n",
    "    replace_list = [\n",
    "            (r\"([0-9]),([0-9])\", r\"\\1\\2\")\n",
    "    ]\n",
    "    return regex_processor(text, replace_list)   \n",
    "\n",
    "# one -> 1\n",
    "def numberconverter(text):\n",
    "    numbers = [\n",
    "            \"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "            \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\",\n",
    "            \"nineteen\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\", \"hundred\", \"thousand\"\n",
    "        ]\n",
    "    digits = [\n",
    "        0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 140, 15,\n",
    "        16, 17, 18, 19, 20, 30, 40, 50, 60, 70, 80, 90, 100, 1000\n",
    "    ]\n",
    "    replace_list = [\n",
    "        (r\"%s\"%n, str(d)) for n,d in zip(numbers, digits)\n",
    "    ]\n",
    "    return regex_processor(text, replace_list)  \n",
    "\n",
    "# remove special characters\n",
    "def specialcharcleaner(text):\n",
    "    replace_list = [\n",
    "            (r\"<.+?>\", r\"\"),\n",
    "            (r\"&nbsp;\", r\" \"),\n",
    "            (r\"&amp;\", r\"&\"),\n",
    "            (r\"&#39;\", r\"'\"),\n",
    "            (r\"/>/Agt/>\", r\"\"),\n",
    "            (r\"</a<gt/\", r\"\"),\n",
    "            (r\"gt/>\", r\"\"),\n",
    "            (r\"/>\", r\"\"),\n",
    "            (r\"<br\", r\"\"),\n",
    "            (r\"[ &<>)(_,;:!?\\+^~@#\\$]+\", r\" \"),\n",
    "            (\"'s\\\\b\", r\"\"),\n",
    "            (r\"[']+\", r\"\"),\n",
    "            (r\"[\\\"]+\", r\"\"),\n",
    "        ]\n",
    "    return regex_processor(text, replace_list)  \n",
    "\n",
    "# handle all HTML tags\n",
    "def HtmlCleaner(text, parser='html.parser'):\n",
    "    bs = BeautifulSoup(text, parser)\n",
    "    text = bs.get_text(separator=\" \")\n",
    "    return text\n",
    "\n",
    "# lemmatizing\n",
    "def lemmatizer(text):\n",
    "    Tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "    Lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    tokens = [Lemmatizer.lemmatize(token) for token in Tokenizer.tokenize(text)]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# stemming\n",
    "def stemmer(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in text.split(\" \")]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply all data cleaning\n",
    "\n",
    "def df_cleaner(df_col):\n",
    "    cleaner_funcs = [\n",
    "        spellchecker,\n",
    "        lowercase,\n",
    "        convertunits,\n",
    "        splitcases,\n",
    "        removewordsplitters,\n",
    "        digitsplitters,\n",
    "        digitcommaremover,\n",
    "        numberconverter,\n",
    "        specialcharcleaner,\n",
    "        HtmlCleaner,\n",
    "        lemmatizer,\n",
    "        stemmer\n",
    "    ]\n",
    "    \n",
    "    for func in cleaner_funcs:\n",
    "        df_col = df_col.apply(func)\n",
    "    return df_col\n",
    "\n",
    "start_train_time = time.time()\n",
    "print(\"--- Cleaning Train Data---\")\n",
    "\n",
    "# Clean train data\n",
    "df_train_1.product_title = df_cleaner(df_train_1.product_title)\n",
    "df_train_1.search_term = df_cleaner(df_train_1.search_term)\n",
    "\n",
    "print(\" :) Train Data Cleaning Finished in %s minutes\" % round(((time.time() - start_train_time)/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    Simpson Strong-Tie 12-Gauge Angle\n",
       "1                    Simpson Strong-Tie 12-Gauge Angle\n",
       "2    BEHR Premium Textured DeckOver 1-gal. #SC-141 ...\n",
       "3    Delta Vero 1-Handle Shower Only Faucet Trim Ki...\n",
       "4    Delta Vero 1-Handle Shower Only Faucet Trim Ki...\n",
       "Name: product_title, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.product_title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      simpson strong tie 12 gaug angl\n",
       "1                      simpson strong tie 12 gaug angl\n",
       "2    behr premium textur deckov 1 gal. sc 141 tugbo...\n",
       "3    delta vero 1 handl shower onli faucet trim kit...\n",
       "4    delta vero 1 handl shower onli faucet trim kit...\n",
       "Name: product_title, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_1.product_title[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_attr.name = str(df_attr.name)\n",
    "df_attr.name = df_cleaner(df_attr.name)\n",
    "df_attr.value = df_cleaner(df_attr.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f0754715602a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pro_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_cleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pro_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-b1e874102556>\u001b[0m in \u001b[0;36mdf_cleaner\u001b[0;34m(df_col)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaner_funcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mdf_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chenbo/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2292\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2293\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2294\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:66124)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b1e874102556>\u001b[0m in \u001b[0;36mstemmer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-b1e874102556>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPorterStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chenbo/anaconda/lib/python3.6/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chenbo/anaconda/lib/python3.6/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_step1b\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0;34m'e'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 lambda stem: (self._measure(stem) == 1 and\n\u001b[0m\u001b[1;32m    377\u001b[0m                               self._ends_cvc(stem))\n\u001b[1;32m    378\u001b[0m             ),\n",
      "\u001b[0;32m/Users/chenbo/anaconda/lib/python3.6/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'*d'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ends_double_consonant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/chenbo/anaconda/lib/python3.6/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36m_ends_double_consonant\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[1;32m    213\u001b[0m         return (\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consonant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "df_pro_desc.product_description = df_cleaner(df_pro_desc.product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train_1.to_csv('../input_clean/train_clean.csv')\n",
    "df_test_1.to_csv('../input_clean/test_clean.csv')\n",
    "df_pro_desc.to_csv('../input_clean/product_descriptions_clean.csv', index=False)\n",
    "df_attr.to_csv('../input_clean/attributes_clean.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
